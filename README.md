# 知识星球内容抓取工具

这个工具可以抓取知识星球的帖子内容，包括文字、图片、PDF文件和评论，并将其保存为Markdown格式。每个帖子会单独保存为一个以发帖时间命名的文件。

## 使用方法

1. 安装依赖库：
```
pip install -r requirements.txt
```

2. 配置访问令牌：
配置文件 `config.py` 中已包含：
- zsxq_access_token - 知识星球的访问令牌
- group_id - 要抓取的星球ID
- user_agent - 浏览器用户代理

3. 运行脚本：
```
python spider.py
```

4. 命令行参数：
```
# 获取帮助信息
python spider.py --help

# 指定开始时间（只抓取该时间之后的帖子）
python spider.py --start_time 2023-01-01T00:00:00Z

# 抓取指定数量的帖子
python spider.py --total 50

# 忽略上次运行时间，重新抓取所有帖子
python spider.py --ignore_last_run

# 修改批次大小和延迟时间
python spider.py --batch_size 10 --delay 3
```

## 输出结果
抓取的内容将保存在 `zsxq_posts` 文件夹中：
- 每个帖子单独保存为一个Markdown文件，文件名格式为 `YYYY-MM-DD_HH-MM-SS_作者名.md`
- `index.md` - 包含所有帖子的索引表格
- `images/` - 所有图片文件（包括帖子和评论中的图片）
- `files/` - 所有PDF等附件文件
- `lastrun.txt` - 记录最后运行的时间，用于增量抓取

## 功能特点
- 分批次抓取帖子，绕过API限制
- 自动抓取每个帖子下的评论，完整保存讨论内容
- 支持增量抓取，只获取新发布的帖子
- 自动记录运行时间，下次运行时自动从上次结束的位置继续
- 每批次抓取后自动等待，避免请求过快
- 每个帖子单独保存为一个文件，包含完整内容和评论
- 所有帖子以时间顺序在索引文件中链接
- 支持下载帖子和评论中的图片和附件
- 自动处理HTML标签，将其转换为纯文本
- 完整的日志记录（保存在 zsxq_spider.log 文件中）
- API错误自动重试机制，最多重试5次
- 智能结束机制：连续3次获取到的帖子数量少于批次大小时自动停止

## 增量抓取说明
脚本会自动记录每次运行的时间，保存在`lastrun.txt`文件中。下次运行时，如果不指定`--start_time`参数，会自动从上次结束的时间开始抓取，避免重复下载已有帖子。

如果需要重新抓取所有帖子，可以使用`--ignore_last_run`参数，或者删除`lastrun.txt`文件。

## 自动停止和错误处理机制
- 当API返回错误时，脚本会自动重试最多5次，每次重试间隔3秒
- 当连续3次获取到的帖子数量少于设定的批次大小时，脚本认为已经获取到所有可用帖子，自动停止抓取
- 如果所有帖子都早于指定的开始时间，脚本也会自动停止
- 所有错误和重试情况都会记录在日志中

## 注意事项
- 请确保您有权限访问该知识星球
- 访问令牌可能会过期，需要定期更新
- 请尊重知识产权，不要非法分发抓取的内容
- 适当调整批次大小和延迟时间，避免请求过快被限制 